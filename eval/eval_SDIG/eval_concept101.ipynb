{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1420b5f9-b995-481a-ab6f-ab29958742a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import clip\n",
    "from transformers import AutoModel\n",
    "from tqdm import tqdm\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipelineLegacy, DDIMScheduler, AutoencoderKL\n",
    "from tide import TIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9cad5d8-0f5e-4158-8a04-8ad1546d2509",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DINOEvaluator:\n",
    "    def __init__(self, device, dino_model='./dino'):\n",
    "        self.device = device\n",
    "        self.model = AutoModel.from_pretrained(dino_model).to(device)  \n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_image_features(self, images):\n",
    "        images = images.to(self.device)  \n",
    "        features = self.model(images)\n",
    "        # features = torch.nn.functional.normalize(features, dim=1)\n",
    "        features = features.last_hidden_state.mean(dim=1)\n",
    "        return features\n",
    "\n",
    "    def img_to_img_similarity(self, src_images, generated_images):\n",
    "        src_img_features = self.get_image_features(src_images)\n",
    "        gen_img_features = self.get_image_features(generated_images)\n",
    "        cos = nn.CosineSimilarity(dim=0)\n",
    "\n",
    "        gen_img_features_expanded = gen_img_features.expand(src_img_features.size(0), -1)\n",
    "        similarities = cos(src_img_features, gen_img_features_expanded)\n",
    "\n",
    "        avg_similarity = similarities.mean().item()\n",
    "\n",
    "        avg_similarity = (avg_similarity + 1) / 2\n",
    "        return avg_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5747a30-8d8f-4ceb-86f6-f66d381fa200",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPEvaluator(object):\n",
    "    def __init__(self, device, clip_model='ViT-B/32') -> None:\n",
    "        self.device = device\n",
    "        self.model, clip_preprocess = clip.load(clip_model, device=self.device)\n",
    "\n",
    "        self.preprocess = clip_preprocess\n",
    "\n",
    "        self.src_preprocess = transforms.Compose([transforms.Normalize(mean=[-1.0, -1.0, -1.0], std=[2.0, 2.0,\n",
    "                                                                                                 2.0])] +\n",
    "                                             clip_preprocess.transforms[:2] +\n",
    "                                             clip_preprocess.transforms[4:])\n",
    "\n",
    "    def tokenize(self, strings: list):\n",
    "        return clip.tokenize(strings).to(self.device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_text(self, tokens: list) -> torch.Tensor:\n",
    "        return self.model.encode_text(tokens)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_images(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        images = self.src_preprocess(images).to(self.device)\n",
    "        return self.model.encode_image(images)\n",
    "\n",
    "    def get_text_features(self, text: str, norm: bool = True) -> torch.Tensor:\n",
    "        tokens = clip.tokenize(text).to(self.device)\n",
    "        text_features = self.encode_text(tokens).detach()\n",
    "        if norm:\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        return text_features\n",
    "\n",
    "    def get_image_features(self, img: torch.Tensor, norm: bool = True) -> torch.Tensor:\n",
    "        image_features = self.encode_images(img)\n",
    "        if norm:\n",
    "            image_features /= image_features.clone().norm(dim=-1, keepdim=True)\n",
    "        return image_features\n",
    "\n",
    "    def img_to_img_similarity(self, src_images, generated_images):\n",
    "        src_img_features = self.get_image_features(src_images)\n",
    "        gen_img_features = self.get_image_features(generated_images)\n",
    "        return (src_img_features @ gen_img_features.T).mean()\n",
    "\n",
    "    def txt_to_img_similarity(self, text, generated_images):\n",
    "        text_features = self.get_text_features(text)\n",
    "        gen_img_features = self.get_image_features(generated_images)\n",
    "        return (text_features @ gen_img_features.T).mean()\n",
    "\n",
    "\n",
    "class ImageDirEvaluator(CLIPEvaluator):\n",
    "    def __init__(self, device, clip_model='ViT-B/32') -> None:\n",
    "        super().__init__(device, clip_model)\n",
    "\n",
    "    def evaluate(self, gen_samples, src_images, target_text):\n",
    "        sim_samples_to_img = self.img_to_img_similarity(src_images, gen_samples)\n",
    "        sim_samples_to_text = self.txt_to_img_similarity(target_text.replace(\"{}\", \"\").replace('-',' '), gen_samples)\n",
    "        return sim_samples_to_img, sim_samples_to_text\n",
    "\n",
    "\n",
    "def load_images_from_folder(folder_path, evaluator):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = evaluator.preprocess(image).unsqueeze(0)\n",
    "            images.append(image)\n",
    "    if images:\n",
    "        return torch.cat(images, dim=0)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10471d92-5db9-4c91-9121-3a7c4440dd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_evaluator = ImageDirEvaluator(device, clip_model='path/to/clip/ViT-B-32.pt')\n",
    "dino_evaluator = DINOEvaluator(device, dino_model='/path/to/dino')\n",
    "\n",
    "data_folder = 'data'\n",
    "source_folder = os.path.join(data_folder,'source')\n",
    "prompts_folder = os.path.join(data_folder, 'prompts')\n",
    "output_folder = 'concept101_output'\n",
    "\n",
    "\n",
    "base_model_path = \"path/to/sd1.5\"\n",
    "vae_model_path = \"path/to/sd1.5/vae\"\n",
    "image_encoder_path = \"models/image_encoder\"\n",
    "ip_ckpt = \"models/tide/tide_sdv1.5.bin\"\n",
    "\n",
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None\n",
    ")\n",
    "ip_model = TIDE(pipe, image_encoder_path, ip_ckpt, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ef0ef6-92e9-4806-8d9d-0a8d3feb5550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "total_clip_i_score = 0\n",
    "total_clip_t_score = 0\n",
    "total_dino_score = 0\n",
    "\n",
    "total_clip_i_score_all = 0\n",
    "total_clip_t_score_all = 0\n",
    "total_dino_score_all = 0\n",
    "image_count = 0\n",
    "\n",
    "\n",
    "total_images = len([f for f in os.listdir(source_folder) if os.path.isdir(os.path.join(source_folder, f))])\n",
    "\n",
    "\n",
    "print(total_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32623046",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=total_images, desc=\"Processing Images\") as pbar:\n",
    "    for sub_folder_name in os.listdir(source_folder):\n",
    "        source_sub_folder = os.path.join(source_folder, sub_folder_name)\n",
    "        if os.path.isdir(source_sub_folder):\n",
    "            src_images = load_images_from_folder(source_sub_folder, clip_evaluator)\n",
    "            src_images4dino = load_images_from_folder(source_sub_folder, dino_evaluator)\n",
    "            if src_images is None:\n",
    "                continue\n",
    "\n",
    "\n",
    "            category_name = sub_folder_name.split('_')[0]\n",
    "\n",
    "\n",
    "            prompt_file_path = os.path.join(prompts_folder, f\"{category_name}.txt\")\n",
    "            with open(prompt_file_path, 'r') as file:\n",
    "                prompts = file.read().strip().split('\\n')\n",
    "\n",
    "            output_sub_folder = os.path.join(output_folder, sub_folder_name)\n",
    "            os.makedirs(output_sub_folder, exist_ok=True)\n",
    "\n",
    "            image = None\n",
    "            for filename in os.listdir(source_sub_folder):\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    image_path = os.path.join(source_sub_folder, filename)\n",
    "                    image = Image.open(image_path).convert(\"RGB\")\n",
    "                    image.resize((256,256))\n",
    "                    break\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for prompt in prompts:\n",
    "                    prompt = prompt.rstrip()\n",
    "                    final_prompt = prompt.replace(\"{}\", category_name)\n",
    "                    generated_images = ip_model.generate(pil_image=image, num_samples=6, num_inference_steps=50,\n",
    "                                                         prompt=final_prompt, scale=0.5)\n",
    "\n",
    "\n",
    "                    prompt_folder_name = prompt.rstrip('.').replace(\" \", \"_\")  \n",
    "                    prompt_output_folder = os.path.join(output_sub_folder, prompt_folder_name)\n",
    "                    os.makedirs(prompt_output_folder, exist_ok=True)\n",
    "\n",
    "                    best_clip_i_score = -float('inf')\n",
    "                    best_clip_t_score = -float('inf')\n",
    "                    best_dino_score = -float('inf')\n",
    "                    best_score = -float('inf')\n",
    "                    best_image = None\n",
    "        \n",
    "\n",
    "                    for i, gen_image in enumerate(generated_images):\n",
    "                        \n",
    "                        image_path = os.path.join(prompt_output_folder, f\"generated_{i+1}.png\")\n",
    "                        gen_image.save(image_path)\n",
    "                        \n",
    "                        gen_sample = clip_evaluator.preprocess(gen_image).unsqueeze(0).to(device)\n",
    "                        sim_samples_to_img, sim_samples_to_text = clip_evaluator.evaluate(gen_sample, src_images, final_prompt)\n",
    "                        clip_i_score = sim_samples_to_img.item()\n",
    "                        clip_t_score = sim_samples_to_text.item()\n",
    "\n",
    "                        gen_sample = dino_evaluator.preprocess(gen_image).unsqueeze(0).to(device)\n",
    "                        similarity = dino_evaluator.img_to_img_similarity(src_images4dino, gen_sample)\n",
    "                        dino_score = similarity\n",
    "\n",
    "                        total_clip_i_score_all += clip_i_score\n",
    "                        total_clip_t_score_all += clip_t_score\n",
    "                        total_dino_score_all += dino_score\n",
    "                        \n",
    "                        if 0.25*dino_score+0.25*clip_i_score+0.5*clip_t_score > best_score:\n",
    "                            best_score = 0.25*dino_score+0.25*clip_i_score+0.5*clip_t_score\n",
    "                            best_clip_i_score = clip_i_score\n",
    "                            best_clip_t_score = clip_t_score\n",
    "                            best_dino_score = dino_score\n",
    "                            best_image = gen_image\n",
    "        \n",
    "                    if best_image:\n",
    "                        \n",
    "                        best_image_path = os.path.join(prompt_output_folder, \"best.png\")\n",
    "                        best_image.save(best_image_path)\n",
    "        \n",
    "                        total_clip_i_score += best_clip_i_score\n",
    "                        total_clip_t_score += best_clip_t_score\n",
    "                        total_dino_score += best_dino_score\n",
    "                        \n",
    "                        image_count += 1\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de095ae1-ca3e-4319-9f34-320a14d2f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "if image_count > 0:\n",
    "    avg_clip_i_score = total_clip_i_score / image_count\n",
    "    avg_clip_t_score = total_clip_t_score / image_count\n",
    "    avg_dino_score = total_dino_score / image_count\n",
    "    print(f\"CLIP-I's best average score: {avg_clip_i_score}\")\n",
    "    print(f\"CLIP-T's best average score: {avg_clip_t_score}\")\n",
    "    print(f\"DINO's best average score: {avg_dino_score}\")\n",
    "    image_count *= 6\n",
    "    avg_clip_i_score = total_clip_i_score_all / image_count\n",
    "    avg_clip_t_score = total_clip_t_score_all / image_count\n",
    "    avg_dino_score = total_dino_score_all / image_count\n",
    "    print(f\"CLIP-I's average score: {avg_clip_i_score}\")\n",
    "    print(f\"CLIP-I's average score: {avg_clip_t_score}\")\n",
    "    print(f\"DINO's average score: {avg_dino_score}\")\n",
    "else:\n",
    "    print(\"No valid images were found for rating calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33ccda6-1680-4e5e-80d6-a67822a1a200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be85bf1-910e-42fe-b6ae-f6f49d6a7f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e2431e-b95d-42a7-9ae7-5196b51ef06d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
